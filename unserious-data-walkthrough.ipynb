{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bde514",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Connecting our Research Question to Data\n",
    "\n",
    "***Does the appearance of JHU in the movies affect application rates?***\n",
    "\n",
    "What data will we need to answer this question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd0b64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- List of JHU appearances in film\n",
    "    - Year of appearance\n",
    "    - Characteristics of the movie - genre, rating, etc.\n",
    "- JHU application rates\n",
    "    - Year of the application rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfeca3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding and accessing the data we need "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b708dad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is usually the hard part, but as a Love Data Week present we've pre-prepared a set of files that can help us answer our research question ‚ù§Ô∏è\n",
    "\n",
    "- **`jhu-titles.csv`**: Titles and Years of titles featuring, or filmed at, JHU, scraped from [Johns Hopkins on Film](https://hub.jhu.edu/2015/10/13/johns-hopkins-on-film/)\n",
    "- **`imdb-title-basics.csv`**: A truncated version of `title.basics.tsv` from [IMDB Datasets](https://datasets.imdbws.com/) including 26 titles that match the Title/Year combination from `jhu-titles.csv`\n",
    "- **`imdb-ratings.csv`**: The original `title.ratings.tsv` from [IMDB Datasets](https://datasets.imdbws.com/), exported as a .csv\n",
    "- **`IPEDS_Raw`**: folder of 20 files, each containing JHU application and acceptance data from one year 2001 - 2020, downloaded from [IPEDS](https://nces.ed.gov/ipeds) (Integrated Postsecondary Education Data System)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e08d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data cleaning considerations\n",
    "\n",
    "While we have pre-prepared files that we can use to answer our research question, they are not 100% ready. We do have some data considerations to think about before our data will be ready to answer our research question:\n",
    "\n",
    "- There are often more than one match in `imdb-title-basics.csv` to the titles listed in `jhu-titles.csv`, so we will have to be careful to ensure we are selecting the right title.\n",
    "- Titles are not always consistent, for example **The Curve (1998)** has the original title **Dead Man's Curve**, so we will need to be careful with our titles during analysis.\n",
    "-  Both `imdb-title-basics.csv` and `jhu-titles.csv` have fields with year information, which we will need to ensure is treated as a date, and not an as arbirtrary numeric value.\n",
    "- Finally, we have data spread across many many files! We have a little bit of work ahead of us to ensure these are combined in a way that makes them meaningful and useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a5c25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing Acceptance Rate Data\n",
    "\n",
    "Prior to the workshop, we downloaded data on applications and acceptances from IPEDS. \n",
    "\n",
    "Let's go over to the [IPEDS Custom Data File page](https://nces.ed.gov/ipeds/datacenter/InstitutionByName.aspx?goToReportId=5&sid=506bb19e-f71f-4863-b838-95a0b656aebf&rtid=5) and walk through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d8665",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Key takeaways:\n",
    "- Our earliest JHU film appearance is 1946,  but we found out that acceptance data is only available from 2001 on, so we'll have to filter out films from prior to 2000 when plotting the data.\n",
    "- Each year downloaded as a separate file. Looks like we'll have to find a way to combine them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d2ada",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's take a look at a couple of the files we downloaded.\n",
    "\n",
    "We'll be using the a Python library called `pandas` to read and prepare this data - to read in a csv to a spreadsheet-like structure called a DataFrame, we'll use the `pandas` function [`read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3e5b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.read_csv(\"IPEDS_Raw/CSV_192026-1007.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972add0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"IPEDS_Raw/CSV_192026-1030.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7c167",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can see that each csv has a single row representing a single year, and the variables we downloaded from IPEDs. But the variable names have different prefixes in each file - we'll have to address this when combining them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3f889",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining IPEDS files\n",
    "\n",
    "To combine our files, we'll use the [`concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) function from pandas, which allows us to append either columns or rows from one DataFrame to another. The axis argument determines whether we 'stack' two DataFrames on top of each other or side by side.\n",
    "\n",
    "axis=0 refers to the rows of the DataFrames, and tells pandas to stack the DataFrames vertically. axis=1 refers to the columns of the DataFrames and tells pandas to stack the DataFrames horizontally.\n",
    "\n",
    "Because we want to stack vertically, we need to make sure the two DataFrames have the same columns and respective data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6f663",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we can use `concat()`, we need to read all our files into dataframes. As we saw above, to use `read_csv()` we need the path to the file. While  we could copy these out manually, Python has a method called `glob()` for listing the contents of a given directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc30da8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the directory path where your CSV files are located. Use a relative path (e.g., './my_csv_files') or an absolute path\n",
    "directory_path = './IPEDS_Raw'\n",
    "\n",
    "# Get a list of all CSV files in the directory. The pattern '*.csv' matches all files ending with .csv\n",
    "ipeds_files = glob.glob(os.path.join(directory_path, '*.csv'))\n",
    "print(ipeds_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb489fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we can loop through all of the files in the directory we designated above and read each in as a pandas dataframe.\n",
    "To make sure each dataframe has the same column names, we'll use the `header` and `names` arguments of `read_csv()` to set standard column names manually as we read each file in.\n",
    "Then, we'll use `concat()` to combine all of the dataframes together into a dataframe called `combined_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d6e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a empty dataframe to insert the data from our csvs into\n",
    "ipeds_df = pd.DataFrame()\n",
    "\n",
    "for file in ipeds_files:\n",
    "    df = pd.read_csv(file, header = 0, names = ['id', 'institution', 'year', 'applicants_total', 'applicants_men', 'applicants_women',\n",
    "                                                'admissions_total', 'admissions_men', 'admissions_women', 'enrolled_total', 'enrolled_men',\n",
    "                                                'enrolled_women', 'enrolled_ft_total', 'enrolled_ft_men', 'enrolled_ft_women',\n",
    "                                                'enrolled_pt_total', 'enrolled_pt_men', 'enrolled_pt_women'])\n",
    "    ipeds_df = pd.concat([df, ipeds_df], ignore_index=True)\n",
    "ipeds_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2918597",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Planning our data preparation\n",
    "\n",
    "Now that all our data is in one place, let's recall our goal: to generate a table with acceptance rate by year. We can use the pandas function [`describe()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html#pandas.DataFrame.describe) to generate a quick summary of our data to see if we want to add anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d66019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df_name.describe() to look at summary stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff73e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling missing data\n",
    "\n",
    "We can calculate the overall acceptance rate from `admissions_total` and `applicants_total`. It looks like we could also break down acceptance rate by gender and full-time status. But weird - the `enrolled_pt` variables have a count of 13, while the others have 20. This is because `count()` doesn't count `NaN` values - so these columns have some missing data. \n",
    "\n",
    "Another way to check for missing data is with the pandas function [`isna()`](https://pandas.pydata.org/docs/reference/api/pandas.isna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db06177",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# check for missing values with df.isna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b3dc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When applied to a DataFrame, `is_na()` returns a DataFrame with the same dimensions with True/False values indicating whether that value is missing. To get see which variables have missing data at a glance, we can *chain* the pandas functions [`any()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.any.html#pandas.Series.any) or [`mean()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.mean.html#pandas.Series.mean) onto `combined_df.isna()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d4fd3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Chain any() onto df.isna() to see columns with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98b548",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Chain mean() onto df.isna() to see percentage of missing data in each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a999bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since the part-time variables are missing a sufficient amount of data, looking at acceptance rates for full-time status won't be too meaningful. But since the other variables are complete, let's go ahead and use the pandas function [`dropna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html#pandas.DataFrame.dropna) to remove columns with missing data. Remember from `concat()`, setting the `axis` argument = 1 specifies that we want to drop columns. specifying axis = 0 would remove rows with missing values - 35% of our dataframe with good data in most columns!\n",
    "\n",
    "The `inplace` argument applies the change to our original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9aaf3b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# use df.dropna() to remove missing values in rows or columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2856d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating the acceptance rate\n",
    "\n",
    "Finally, let's calculate the variables for acceptance rate.\n",
    "\n",
    "The syntax to create a new column in pandas is to assign some value (in this case, the quotient of two other columns) to `df_name[new_column_name]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9cbdc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create acceptance rate variables for overall, men, and women\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c01dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finalizing the dataframe\n",
    "\n",
    "We only need some of these variables for our analysis. To keep things clean, let's just keep `year`, `acceptance_rate`, `acceptance_rate_men`, and `acceptance_rate_women`. \n",
    "\n",
    "The syntax for subsetting in pandas is df[column name], where column_name can also be a list of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677ac90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create a new dataframe called acceptance_rates with only the year and acceptance rate variable \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f1cb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finalizing the dataframe\n",
    "\n",
    "We can sort the dataframe with the function [`sort_values()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.sort_values.html#pandas.Series.sort_values), with the `by` argument set to the column we want to sort by, `inplace` set to true to modify the original dataframe, and `ignore_index` set to true to re-number the rows in our new order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7eaee6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# use df.sortvalues() to sort the new dataframe by year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a107887-e5cf-48a2-b120-ebc7d77b22a7",
   "metadata": {},
   "source": [
    "## Preparing JHU titles\n",
    "\n",
    "We want to explore titles filmed at, or about, Johns Hopkins, as described in the HUB article [Johns Hopkins on Film](https://hub.jhu.edu/2015/10/13/johns-hopkins-on-film/), and combine it with title information from IMDB:\n",
    "\n",
    "<div> <img src=\"images/hopkins-on-film.png\" width=\"500\"/> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d7ee9-a568-46c1-be6f-e2b750543c8d",
   "metadata": {},
   "source": [
    "### Load JHU and IMDB data\n",
    "First we will read in our JHU titles, by importing `jhu-titles.csv`. Then let's read in our IMDb data. First we will start with basic information about the titles, by importing the `imdb-title-basics.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c52f4bf2-004c-4f38-a6c6-e75a997c0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the IMDb and JHU title data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b14bd-8ef0-4f5a-9bbe-8d99a6782702",
   "metadata": {},
   "source": [
    "From the IPEDS data, you remember we used the convention `variable_name` for columns, where the variable titles were lowercase, and separated by spaces for multi-word variables. This is otherwise known as `snake_case` üêç. Let's do the same here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7de3093-da5c-4006-87a3-aa9b0569386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the variable names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37139185-4bd5-4d25-b06f-6b91bdb93d6a",
   "metadata": {},
   "source": [
    "### Clean our variable names\n",
    "We see that our variables are in camelCase and we want to convert them to snake_case. We can write a little helper to do that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a59a6809-b38b-498f-a939-a22baad6d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a short function to convert camel case to snake case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b716323-1c92-4299-a942-093e6523f0e5",
   "metadata": {},
   "source": [
    "What's happening in this little function?\n",
    "\n",
    "Remember our goal is to convert `titleType` to `title_type`. Our function takes a string, for example `titleType` as an argument, and returns the snake case format of that string.\n",
    "\n",
    "- We use Python's `re` module, which stands for \"regular expression\". Regular Expressions (or Regex for short) provide us a systematic way of finding text matches in a string, and lets us do things with those matches.\n",
    "  \n",
    "- We can use the `re.sub()` function which can replace a match, with a new value. For example, `re.sub('apple', 'banana', 'apple bread')` would replace the word \"apple\" with the word \"banana\" in `apple bread` so we end up with `banana bread` (yum!)\n",
    "- In our function, `r([A-Z])` matches any capital letter (in our case `T`). The `r` prefix indicates we are using a regular expression. The `[A-Z]` says find any upper case between the letters A to Z (so all of them). By putting everything between parentheses `([A-Z])` that says that if we find something, we want to keep it for later. This is called a \"capturing group\".\n",
    "- Our second argument is the replacement. `_\\1` replaces the `T` with an underscore + the same letter it found (`\\1` is the \"capturing group, and it contains our first match), so we end up with `title_Type`.\n",
    "- Now we can use the `lower.()` function to return the result in lowercase: `title_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9a283e7-9d24-4b2b-ba58-3b0352b0ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert imdb_titles to snake_case using our camel_to_snake function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feea620-8feb-4fd6-b40e-f3d69b58e2f2",
   "metadata": {},
   "source": [
    "### Merge JHU title data with IMDb data¬∂\n",
    "Now let's try to bring our data together! We would like add the variables from `imdb_titles` to `jhu_titles` when an IMDb title matches a title in `jhu_titles`.\n",
    "\n",
    "We can do this using a `pandas` function - `pd.merge()`. We are joining on multiple columns (title and year). In SQL this is known as a \"compound key\".\n",
    "\n",
    "We use a left join into `jhu_titles`. This means that:\n",
    "\n",
    "- We keep every row from the left table (`jhu_titles`).\n",
    "- If you can find matching rows in the right table (`imdb_titles`), attach them.\n",
    "- If you can‚Äôt, keep the row anyway and fill the right side with NaNs.\n",
    "\n",
    "There are two valid title columns in `imdb_titles` to choose: `original_title` and `primary_title`. Let's start with `original_title` and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "712de0cc-fd9d-4906-bfe0-3b53180dcbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join jhu_titles and imdb_titles on title into jhu_imdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc27f78-df97-489b-9b19-3bc16164cf7c",
   "metadata": {},
   "source": [
    "That looks good! Ok now let's add the ratings to our `jhu_imdb` dataframe. Both our `jhu_imdb` and `imdb_ratings` dataframes contain a uniquely identifying key: `tconst`. Each title has a unique `tconst`. We will again use a `pd.merge` to join these two dataframes on our unique key. In SQL this is called a \"primary key\". If `tconst` was our index (instead of the integer 0, 1, 2, 3, ... labeling each row), then we would use a pandas join, but since it's a regular column in both, then we will use `pd.merge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dda19311-e006-4fc9-88a2-6649347b4af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join jhu_imdb and imdb_ratings using pandas merge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd268d8-c27f-44df-a6aa-397ab749e774",
   "metadata": {},
   "source": [
    "That looks great, but there are a few data quality issues. Can you see them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed009b-66e1-45c0-8840-6c2e53b4902e",
   "metadata": {},
   "source": [
    "### Data cleaning JHU titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98ae2073-8a43-4ef7-b99f-4f7221a5917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types for jhu_imdb_ratings to ensure consistency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da4006-22de-4713-b8ff-46aa36b996df",
   "metadata": {},
   "source": [
    "- `year` is an int64\n",
    "- `start_year` and `end_year` are float64\n",
    "- No year variable is in a proper date-time format!\n",
    "- Votes are a count, so they shound be an integer ‚Äì we don't expect half votes.\n",
    "- Our genre isn't tidy, we have more than once genre listed in a single field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ec4cf9a-e6ff-4735-84ec-a835fbb968de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our years to a numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523db9e-82dc-467c-b2fc-f2c4757f5ec5",
   "metadata": {},
   "source": [
    "This is a good time to check the number of rows in our dataframe. Do we have as many rows as we have titles in `jhu_titles`? More? Less?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e815d48-51f9-493e-a8c6-689e3f2d82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare jhu_titles and jhu_imdb_ratings length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6be2e-32a9-482d-80e3-17d73fd52f57",
   "metadata": {},
   "source": [
    "Looks like we have more rows than we have titles, so we need to do some filtering. Sometimes, we can't automate everything. Here we need to rely on our knowledge of the titles to filter out rows that don't match. Let's take a look at the complete dataframe, and then strategize about how to filter out rows that don't correspond to a title in jhu_titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd8c85c3-6b8c-4f7c-b9be-a989f000484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore jhu_imdb_ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6decca0-61ed-4e4a-a7c9-7be1b8439709",
   "metadata": {},
   "source": [
    "Just by looking at the dataframe, a few things stand out:\n",
    "\n",
    "- \"Veep\" and \"House of Cards\" are TV Series, so we don't need individual episodes, just the IMDB entry for the series.\n",
    "- We know that \"The Social Network\", \"The Invasion\", and \"Head of State\" are all movies.\n",
    "\n",
    "With that information alone, we can filter out our extraneous rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0db17ad8-e56b-4eb1-9c6a-43f57010c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out individual episodes and titles non-movie titles for The Social Network,\n",
    "# The Invasion, and Head of State\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4920d9f-a26c-42b1-943c-32e81b3fbb88",
   "metadata": {},
   "source": [
    "We have now filtered our data down to the 13 titles listed in `jhu_titles`! The last thing to do is to ensure our data is tidy, that is:\n",
    "- One observation per row,\n",
    "- One variable per column,\n",
    "- One value per cell.\n",
    "\n",
    "We see that's not the case, `genre` has one or more values per cell. Let's clean that up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9446876-4032-4c20-b987-3bfea1d4a86a",
   "metadata": {},
   "source": [
    "We are going to chain our data cleaning using the `.` operator, since we have a number of steps.\n",
    "1. We create a new column (`genre`) and assign it the values of `genres` split into a list of strings.\n",
    "2. We `explode` genre, which means we create a new row for each list element present in `genre`, and copy the other variables into that new row.\n",
    "3. Since our genres were originally in a single string, like: `Comedy, Drama, Sci-Fi`, when we split them, \"Drama\" and \"Sci-Fi\" still had a space in front of them, and we want to remove that using `str.strip()`. Do operate on each row, and remove the `genre` spaces for that row, we write an anonymous function, otherwise know as a lambda expression. This is an unnamed function, since we only need to use it once, and don't need to preserve it for use elsewhere.\n",
    "4. Since we have duplicate rows, our index might look something like: `[0, 1, 1, 2, 2, 2, 3, 3, 4, 5, 5]`, so we need to reset it so that the index increments by one for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ba25ef4-67ef-41cd-b8b4-fd779f18c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each list of genres, by title, into individual rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d735f6-a8e0-45ca-b13b-10e6a9cee981",
   "metadata": {},
   "source": [
    "## Visualizing our research question:\n",
    "\n",
    "> Does the appearance of JHU in the movies affect acceptance rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce14f1c-c352-4716-9945-f7b723783d83",
   "metadata": {},
   "source": [
    "Before we can begin visualizing our data, we have a little data cleaning left. We know we don't have acceptance_rates for every title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9be5cdfb-95a2-43da-95bc-33f88f8e35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find earliest acceptance rate in acceptance_rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1259d-3376-4b2f-8e47-e65f8c51ebd7",
   "metadata": {},
   "source": [
    "We need to filter our data down to titles that were released on years we have acceptance_rate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54fc61a1-7c9a-43c9-b6d4-b2684b7678d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows of jhu_clean to years that we have acceptance_rate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232640d4-c87d-49ab-b8d9-363df73f98c8",
   "metadata": {},
   "source": [
    "Now we can begin to visualize our research question. There are many approaches we could take, and there is no right answer as to which would be the most effective. We will demonstrate one pathway, of many. \n",
    "\n",
    "We need to display JHU's annual acceptance rates, and then display the titles that were released during those years, overlayed, so we can draw a comparison between acceptance rates and the titles (and attributes of the titles).\n",
    "\n",
    "We have a little data cleaning left, however. We need to convert acceptance_rates from wide to long format. Let's take a look and see what that means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42956851-b1a4-452d-bc01-a9c14b018955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore acceptance_rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ebdfe-34c2-4d46-ab9b-bf5293de9dd7",
   "metadata": {},
   "source": [
    "Each acceptance rates for women, men, and overall each have their own column. In this way, the dataframe is wide (the variables are spread across subsequent columns). We instead want a single column for acceptance rates, and a single column indicating the class (women, men, or overall). When we spread our variables over subsequent rows, our dataframe is \"long\".\n",
    "\n",
    "We can do this using the pandas function `melt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c115b885-ee08-41ca-910d-90b70401c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert acceptance_rates from wide to long using pd.melt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0073b-ff8c-41bb-885b-988b06f3a611",
   "metadata": {},
   "source": [
    "Now we can begin by visualizing acceptance rates by year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab42d9f-4d64-4244-89f1-40faf6e0c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lineplot of acceptance rates by year\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "palette = {'Overall': (.2, .2, .2, .9),\n",
    "           'Women': (.2, .2, .8, .2),\n",
    "           'Men': (.2, .8, .2, .2)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dcee0f-bb2a-4abd-8914-d5f8aa25a61d",
   "metadata": {},
   "source": [
    "### Display titles with acceptance rates\n",
    "If we want to plot the titles, we need a shared y-axis attribute, in this case acceptance_rate. We need merge our acceptance rates into our `jhu_title_current` dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2518d-03ed-4a90-a464-c6465af1ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge jhu_titles_current and acceptance_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fbec1-d6a6-41a4-aa42-ae34693f26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display titles\n",
    "\n",
    "palette = {'Overall': (.2, .2, .2, .9),\n",
    "           'Women': (.2, .2, .8, .2),\n",
    "           'Men': (.2, .8, .2, .2)}\n",
    "\n",
    "title_type_palette = {\n",
    "    'movie': '#3498DB',        \n",
    "    'tvSeries': '#E67E22',     \n",
    "    'tvMiniSeries': '#9B59B6', \n",
    "    'tvMovie': '#1ABC9C'       \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bce95b-7666-476f-a34e-61be5ad288d8",
   "metadata": {},
   "source": [
    "It seems like there might be a (spurious) correlation between title type and acceptance rate. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9a738-6a4d-4436-afb9-f30693c3a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average overall acceptance rate, by title type\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
